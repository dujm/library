{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58642f9f-8272-4838-bb69-6e3c147244e7",
   "metadata": {},
   "source": [
    "# Scify is back: Version A (using Langchain Anthropic package) \n",
    "\n",
    "### Who is Scify?\n",
    "An LLM chatbot who is expected to \n",
    "1) find books on [Project Gutenberg](https://www.gutenberg.org/)\n",
    "2) find the txt url for each book\n",
    "\n",
    "\n",
    "### How was Scify?\n",
    " * In [my previous experiemnt](https://github.com/dujm/book-reader/blob/master/notebooks/03-chatbot.ipynb), I used `llama2 7B` (Ollma ID `78e26419b446`) \n",
    " * Scify did a good job in finding the relevant books, but failed to find the relevant text urls.\n",
    "\n",
    "<br>\n",
    "\n",
    "### How is Scify different now? \n",
    " * In this experiment, I used Anthropic's Claude-3 model (`claude-3-opus-20240229`).\n",
    "\n",
    "#### Result\n",
    " * Claude 3 helped Scify to significantly improve its performance (i.e. no hallucination).\n",
    " * Scify did not only find the relevant books, but also the correct txt urls. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf49e49-b97e-49a7-9abf-f0fddb7dcbbf",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33d64d-4680-49a6-996c-64e8285f518d",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e70ee94c-5786-4667-aa1f-256abe659bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a4333f-afc5-4712-85f9-e7c8c08b00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# process Python abstract syntax\n",
    "import ast\n",
    "\n",
    "# pretty print\n",
    "from pprint import pprint\n",
    "\n",
    "# langchain\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "# langchain_anthropic\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "\n",
    "# load the .env file (for saving API keys, see details in `.env_example file`)\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# assign loaded API keys to variables\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73da92-71fa-42ab-bed6-1e3e8a31ca23",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f8c0730-0b05-4f22-aef5-2ccc1b8bf88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------#\n",
    "# model related\n",
    "#----------------------#\n",
    "# model\n",
    "llm_model_id = 'claude-3-opus-20240229'\n",
    "\n",
    "\n",
    "#----------------------#\n",
    "# query related\n",
    "#----------------------#\n",
    "# topic \n",
    "topic = \"Science Fiction\"\n",
    "\n",
    "# number of results I want to get\n",
    "n= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987204f-a43f-452b-932b-f06bbd4a150b",
   "metadata": {},
   "source": [
    "# Build an LLM Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b64683-f141-4f5a-bd65-148393441dd0",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486c149c-0ffb-4a19-823e-1515c4444743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "template = \"\"\"\n",
    "You are a friendly chatbot assistant that responds in a conversational\n",
    "manner to users questions. Keep the answers short, unless specifically\n",
    "asked by the user to elaborate on something.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2bc54-db20-4f09-abfe-b3ac3315d49d",
   "metadata": {},
   "source": [
    "### Invite an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe151a1-61ec-4875-ac9d-4a52f785b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(model=llm_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8ca16-1377-495d-ae9f-527ddc38e43d",
   "metadata": {},
   "source": [
    "### Create a Langchain workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a43e66-ea2b-4838-b786-34433dc114fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7411ca85-a536-423e-be28-ff63cf0a14e7",
   "metadata": {},
   "source": [
    "# Start Chatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f6a240-ae31-4b80-b87d-24284cd65616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/j/miniconda3/envs/library/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "question = f\"\"\"\n",
    "     Please go to 'https://www.gutenberg.org/', search for  {topic}\n",
    "     list the most popular {n} books and their matched Plain Text UTF-8 links on 'https://www.gutenberg.org/'\n",
    "     Respond with the book title and links.\n",
    "     For example, the most popular Book is\n",
    "     ['The Time Machine': 'https://www.gutenberg.org/cache/epub/35/pg35.txt']\n",
    "     Return a pretty list of python readable form. \n",
    "\"\"\"\n",
    "response = llm_chain({question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603eb118-fa02-498b-8d9b-326b15b47ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to text \n",
    "response_text = response['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3b0e8-fd8b-4174-b689-18b65674c685",
   "metadata": {},
   "source": [
    "### ResultÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad28ec1-ac77-4076-a0cf-3ecd6ba214a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['The War of the Worlds', 'https://www.gutenberg.org/files/36/36-0.txt'],\n",
      " ['The Time Machine', 'https://www.gutenberg.org/files/35/35-0.txt'],\n",
      " ['Flatland: A Romance of Many Dimensions',\n",
      "  'https://www.gutenberg.org/files/201/201-0.txt'],\n",
      " ['The Poison Belt', 'https://www.gutenberg.org/files/126/126-0.txt'],\n",
      " ['A Princess of Mars', 'https://www.gutenberg.org/files/62/62-0.txt'])\n"
     ]
    }
   ],
   "source": [
    "# get clean result\n",
    "results =\"\".join(response_text.splitlines()[3:n+3]).lstrip()\n",
    "results\n",
    "\n",
    "# find out what current syntax grammar looks like programmatically (in this case, turn a string into a list)\n",
    "result_display= ast.literal_eval(results )\n",
    "\n",
    "# pretty print\n",
    "pprint(result_display)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "library",
   "language": "python",
   "name": "library"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
